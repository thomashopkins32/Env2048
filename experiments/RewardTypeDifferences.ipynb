{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee2bcfa",
   "metadata": {},
   "source": [
    "# **Reward Type Differences in 2048**\n",
    "Thomas Hopkins\n",
    "\n",
    "## **Reward Types**\n",
    "Here are the 3 different reward types currently offered in Env2048:\n",
    "1. `score`\n",
    "    - Uses the normal score from the original game.\n",
    "    - This is the sum of all tiles created by merging that occurred after an action.\n",
    "    - Example: If two 8 tiles are merged after an action, then the reward is +16.\n",
    "2. `survival`\n",
    "    - Moves that change the current state have reward +1.0.\n",
    "    - Moves that don't change the current state have reward -0.1.\n",
    "    - Moves that cause game over have reward 0.0.\n",
    "3. `milestone`\n",
    "    - Moves that reach a never before seen tile have reward +10.0\n",
    "    - All other moves have reward 0.0.\n",
    "\n",
    "## **Learning Algorithm**\n",
    "We will use Vanilla Policy Gradient (with GAE-Lambda).\n",
    "\n",
    "## **Experiments**\n",
    "We will train a new VPG model on each type of environment for 10,000 games.\n",
    "\n",
    "The parameters used for the model may differ for each reward type since some rewards are sparser than others.\n",
    "\n",
    "Plotting the reward-per-episode curves should give us an idea of which reward type is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76c13f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import scipy.signal\n",
    "from torch.distributions.categorical import Categorical\n",
    "from env2048.env import Env2048\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def add_length_to_shape(length, shape=None):\n",
    "    '''\n",
    "    Combines an arbitrary shape with a preferred length.\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        size of first axis\n",
    "    shape : tuple[int], optional\n",
    "        size of the rest of the axes\n",
    "    '''\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class TrajectoryBuffer:\n",
    "    def __init__(self, obs_shape, action_shape, size, discount=0.99, lam=0.95):\n",
    "        '''\n",
    "        Stores the trajectories that the agent takes up to the buffer size.\n",
    "        It will store for each step in the environment:\n",
    "            - observation\n",
    "            - immediate reward\n",
    "            - action taken\n",
    "            - probability of selecting that action (according to policy)\n",
    "            - perceived value of the observation\n",
    "        When the trajectory is finished it will compute:\n",
    "            - discounted reward to go\n",
    "            - discounted lambda advantage\n",
    "        The buffer can be emptied by calling the `get()` method\n",
    "        '''\n",
    "        self.obs_buf = np.zeros(add_length_to_shape(size, obs_shape),\n",
    "                                dtype=np.float32)\n",
    "        self.act_buf = np.zeros(add_length_to_shape(size, action_shape),\n",
    "                                dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.start_ptr = 0\n",
    "        self.size = size\n",
    "\n",
    "        self.discount = discount\n",
    "        self.lam = lam\n",
    "\n",
    "    def store(self, obs, action, reward, logp, value):\n",
    "        ''' Store a single step in the buffer '''\n",
    "        if self.ptr == self.size:\n",
    "            print('Cannot store current step. Buffer is full.')\n",
    "            return\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = action\n",
    "        self.rew_buf[self.ptr] = reward\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.val_buf[self.ptr] = value\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_trajectory(self, last_val=0.0):\n",
    "        ''' Computes the return and advantage per step in trajactory '''\n",
    "        path_slice = slice(self.start_ptr, self.ptr)\n",
    "        rewards = np.append(self.rew_buf[path_slice], last_val)\n",
    "        values = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # GAE-Lambda advantage\n",
    "        deltas = rewards[:-1] + self.discount * values[1:] - values[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas,\n",
    "                                                   self.discount * self.lam)\n",
    "        # Rewards-to-go\n",
    "        self.rew_buf[path_slice] = discount_cumsum(rewards,\n",
    "                                                   self.discount)[:-1]\n",
    "        self.start_ptr = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        ''' Empties the buffer into something useable for learning '''\n",
    "        if self.ptr != self.size:\n",
    "            print('ERROR: buffer not full')\n",
    "            return\n",
    "        self.ptr = 0\n",
    "        self.start_ptr = 0\n",
    "        # advantage normalization (LOOK THIS UP!)\n",
    "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = {'obs': self.obs_buf, 'act': self.act_buf, 'ret': self.ret_buf,\n",
    "                'adv': self.adv_buf, 'logp': self.logp_buf}\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nodes_per_layer, activation='relu'):\n",
    "        '''\n",
    "        A basic multi-layered perceptron using PyTorch.\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes_per_layer : List[int]\n",
    "            number of nodes per layer in the network\n",
    "        activation : str, optional\n",
    "            description of activation to use in between layers\n",
    "            supports: {'sigmoid', 'relu', 'tanh'}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        activ_func = None\n",
    "        if activation == 'relu':\n",
    "            activ_func = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            activ_func = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            activ_func = nn.Tanh()\n",
    "        self.mlp = nn.Sequential()\n",
    "        # game state is 2D tensor but we need 1D tensor\n",
    "        nodes_per_layer[0] = nodes_per_layer[0] ** 2\n",
    "        # add layers\n",
    "        for i in range(1, len(nodes_per_layer)):\n",
    "            self.mlp.append(nn.Linear(nodes_per_layer[i-1],\n",
    "                                    nodes_per_layer[i]))\n",
    "            if activ_func is not None and i != len(nodes_per_layer)-1:\n",
    "                self.mlp.append(activ_func)\n",
    "                \n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        else:\n",
    "            x = torch.flatten(x)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class ActorCriticMLP:\n",
    "    ''' Actor/Critic that performs actions and makes value estimates '''\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = MLP([obs_dim, 128, 64, act_dim])\n",
    "        self.critic = MLP([obs_dim, 128, 64, 1])\n",
    "\n",
    "    def distribution(self, obs):\n",
    "        ''' Returns the current policy distribution over the observation '''\n",
    "        return Categorical(logits=self.actor(obs))\n",
    "\n",
    "    def policy(self, obs, act=None):\n",
    "        ''' Returns an action given the observation '''\n",
    "        pi = self.distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a\n",
    "\n",
    "    def value(self, obs):\n",
    "        ''' Returns the perceived value of the observation '''\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def step(self, obs):\n",
    "        ''' Returns the action, value, and logp_a for the observation '''\n",
    "        pi, _ = self.policy(obs)\n",
    "        a = pi.sample()\n",
    "        logp = pi.log_prob(a)\n",
    "        v = self.value(obs)\n",
    "        return a.item(), v.item(), logp.item()\n",
    "\n",
    "\n",
    "class VPG:\n",
    "    ''' Vanilla Policy Gradient Algorithm '''\n",
    "    def __init__(self, buffer_size=500, discount=0.99, pi_lr=0.0003, v_lr=0.001, lam=0.97):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.discount = discount\n",
    "        self.pi_lr = pi_lr\n",
    "        self.v_lr = v_lr\n",
    "        self.lam = lam\n",
    "\n",
    "    def compute_loss_pi(self, data):\n",
    "        obs = data['obs']\n",
    "        act = data['act']\n",
    "        adv = data['adv']\n",
    "        logp_old = data['logp']\n",
    "\n",
    "        pi, logp = self.ac.policy(obs, act=act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        return loss_pi\n",
    "\n",
    "    def compute_loss_val(self, data):\n",
    "        obs = data['obs']\n",
    "        ret = data['ret']\n",
    "        return ((self.ac.value(obs) - ret) ** 2).mean()\n",
    "\n",
    "    def update(self):\n",
    "        ''' Updates policy and value parameters via backprop '''\n",
    "        data = self.buffer.get()\n",
    "\n",
    "        self.pi_optim.zero_grad()\n",
    "        pi_loss = self.compute_loss_pi(data)\n",
    "        pi_loss.backward()\n",
    "        self.pi_optim.step()\n",
    "\n",
    "        for i in range(self.train_v_iters):\n",
    "            self.v_optim.zero_grad()\n",
    "            v_loss = self.compute_loss_val(data)\n",
    "            v_loss.backward()\n",
    "            self.v_optim.step()\n",
    "\n",
    "    def train(self, env_func, epochs=250, train_v_iters=80):\n",
    "        ''' Train an agent on the given environment '''\n",
    "        all_ep_returns = []\n",
    "        all_ep_scores = []\n",
    "        env = env_func()\n",
    "        self.buffer = TrajectoryBuffer(env.observation_space.shape,\n",
    "                                       env.action_space.shape,\n",
    "                                       self.buffer_size,\n",
    "                                       discount=self.discount,\n",
    "                                       lam=self.lam)\n",
    "        self.ac = ActorCriticMLP(env.observation_space.shape[0],\n",
    "                                 env.action_space.n)\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.pi_optim = Adam(self.ac.actor.parameters(), lr=self.pi_lr)\n",
    "        self.v_optim = Adam(self.ac.critic.parameters(), lr=self.v_lr)\n",
    "        o = env.reset()\n",
    "        ep_ret = 0\n",
    "        ep_len = 0\n",
    "        for k in tqdm(range(epochs), total=epochs):\n",
    "            for t in range(self.buffer_size):\n",
    "                with torch.no_grad():\n",
    "                    a, v, logp = self.ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "                next_o, r, done, _ = env.step(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "                self.buffer.store(o, a, r, logp, v)\n",
    "                o = next_o\n",
    "\n",
    "                buffer_full = t == self.buffer_size - 1\n",
    "\n",
    "                if done or buffer_full:\n",
    "                    if buffer_full:\n",
    "                        with torch.no_grad():\n",
    "                            _, v, _ = self.ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                    else:\n",
    "                        v = 0.0\n",
    "                    self.buffer.finish_trajectory(last_val=v)\n",
    "                    ep_score = env.score\n",
    "                    o = env.reset()\n",
    "                    all_ep_returns.append(ep_ret)\n",
    "                    all_ep_scores.append(ep_score)\n",
    "                    ep_ret = 0\n",
    "                    ep_len = 0\n",
    "            self.update()\n",
    "        return all_ep_returns, all_ep_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e5e91",
   "metadata": {},
   "source": [
    "### `score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ae9b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                            | 23/1000 [00:49<35:04,  2.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7862/1273767428.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0menv_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mEnv2048\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvpg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVPG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpi_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mscore_ep_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_ep_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_v_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_v_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_7862/1061198585.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env_func, epochs, train_v_iters)\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0mep_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                     \u001b[0mep_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_ep_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ep_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7862/1061198585.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "discount = 0.99\n",
    "lam = 0.97\n",
    "buffer_size = 2000 # estimate of the number of steps in an episode\n",
    "pi_lr = 0.0003\n",
    "v_lr = 0.001\n",
    "train_v_iters = 80\n",
    "epochs = 1000\n",
    "env_func = lambda : Env2048(size=4, reward_type='score')\n",
    "vpg = VPG(buffer_size=buffer_size, discount=discount, pi_lr=pi_lr, v_lr=v_lr, lam=lam)\n",
    "score_ep_returns, score_ep_scores = vpg.train(env_func, epochs=epochs, train_v_iters=train_v_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9432334",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(score_ep_returns)), score_ep_returns)\n",
    "plt.title('Episode return curve for `score`')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9be4be",
   "metadata": {},
   "source": [
    "### `survival`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b73610",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.99\n",
    "lam = 0.97\n",
    "buffer_size = 2000 # estimate of the number of steps in an episode\n",
    "pi_lr = 0.0003\n",
    "v_lr = 0.001\n",
    "train_v_iters = 80\n",
    "epochs = 1000\n",
    "env_func = lambda : Env2048(size=4, reward_type='survival')\n",
    "vpg = VPG(buffer_size=buffer_size, discount=discount, pi_lr=pi_lr, v_lr=v_lr, lam=lam)\n",
    "survival_ep_returns, survival_ep_scores = vpg.train(env_func, epochs=epochs, train_v_iters=train_v_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(survival_ep_returns)), survival_ep_returns)\n",
    "plt.title('Episode return curve for `survival`')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ea880",
   "metadata": {},
   "source": [
    "### `milestone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.99\n",
    "lam = 0.97\n",
    "buffer_size = 2000 # estimate of the number of steps in an episode\n",
    "pi_lr = 0.0003\n",
    "v_lr = 0.001\n",
    "train_v_iters = 80\n",
    "epochs = 1000\n",
    "env_func = lambda : Env2048(size=4, reward_type='milestone')\n",
    "vpg = VPG(buffer_size=buffer_size, discount=discount, pi_lr=pi_lr, v_lr=v_lr, lam=lam)\n",
    "milestone_ep_returns, milestone_ep_scores = vpg.train(env_func, epochs=epochs, train_v_iters=train_v_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eb83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(milestone_ep_returns)), milestone_ep_returns)\n",
    "plt.title('Episode return curve for `milestone`')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1151d1c9",
   "metadata": {},
   "source": [
    "## **Results**\n",
    "Here is a plot of game scores. This is the best way to compare methods since episode lengths can be arbitrarily long if the same action that doesn't change state is repeated forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f62b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(score_ep_scores)), score_ep_scores, label='score')\n",
    "plt.plot(range(len(survival_ep_scores)), survival_ep_scores, label='survival')\n",
    "plt.plot(range(len(milestone_ep_scores)), milestone_ep_scores, label='milestone')\n",
    "plt.title('Game scores for training on different reward types')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
