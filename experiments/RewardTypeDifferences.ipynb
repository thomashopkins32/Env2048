{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b44faf8",
   "metadata": {},
   "source": [
    "# **Reward Type Differences in 2048**\n",
    "Thomas Hopkins\n",
    "\n",
    "## **Reward Types**\n",
    "Here are the 3 different reward types currently offered in Env2048:\n",
    "1. `score`\n",
    "    - Uses the normal score from the original game.\n",
    "    - This is the sum of all tiles created by merging that occurred after an action.\n",
    "    - Example: If two 8 tiles are merged after an action, then the reward is +16.\n",
    "2. `survival`\n",
    "    - Moves that change the current state have reward +1.0.\n",
    "    - Moves that don't change the current state have reward -0.1.\n",
    "    - Moves that cause game over have reward 0.0.\n",
    "3. `milestone`\n",
    "    - Moves that reach a never before seen tile have reward +10.0\n",
    "    - All other moves have reward 0.0.\n",
    "\n",
    "## **Learning Algorithm**\n",
    "We will use Vanilla Policy Gradient (with GAE-Lambda).\n",
    "\n",
    "## **Experiments**\n",
    "We will train a new VPG model on each type of environment for 10,000 games.\n",
    "\n",
    "The parameters used for the model may differ for each reward type since some rewards are sparser than others.\n",
    "\n",
    "Plotting the reward-per-episode curves should give us an idea of which reward type is ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0702fcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import scipy.signal\n",
    "from torch.distributions.categorical import Categorical\n",
    "from env2048.env import Env2048\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def add_length_to_shape(length, shape=None):\n",
    "    '''\n",
    "    Combines an arbitrary shape with a preferred length.\n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        size of first axis\n",
    "    shape : tuple[int], optional\n",
    "        size of the rest of the axes\n",
    "    '''\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class TrajectoryBuffer:\n",
    "    def __init__(self, obs_shape, action_shape, size, discount=0.99, lam=0.95):\n",
    "        '''\n",
    "        Stores the trajectories that the agent takes up to the buffer size.\n",
    "        It will store for each step in the environment:\n",
    "            - observation\n",
    "            - immediate reward\n",
    "            - action taken\n",
    "            - probability of selecting that action (according to policy)\n",
    "            - perceived value of the observation\n",
    "        When the trajectory is finished it will compute:\n",
    "            - discounted reward to go\n",
    "            - discounted lambda advantage\n",
    "        The buffer can be emptied by calling the `get()` method\n",
    "        '''\n",
    "        self.obs_buf = np.zeros(add_length_to_shape(size, obs_shape),\n",
    "                                dtype=np.float32)\n",
    "        self.act_buf = np.zeros(add_length_to_shape(size, action_shape),\n",
    "                                dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.start_ptr = 0\n",
    "        self.size = size\n",
    "\n",
    "        self.discount = discount\n",
    "        self.lam = lam\n",
    "\n",
    "    def store(self, obs, action, reward, logp, value):\n",
    "        ''' Store a single step in the buffer '''\n",
    "        if self.ptr == self.size:\n",
    "            print('Cannot store current step. Buffer is full.')\n",
    "            return\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = action\n",
    "        self.rew_buf[self.ptr] = reward\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.val_buf[self.ptr] = value\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_trajectory(self, last_val=0.0):\n",
    "        ''' Computes the return and advantage per step in trajactory '''\n",
    "        path_slice = slice(self.start_ptr, self.ptr)\n",
    "        rewards = np.append(self.rew_buf[path_slice], last_val)\n",
    "        values = np.append(self.val_buf[path_slice], last_val)\n",
    "\n",
    "        # GAE-Lambda advantage\n",
    "        deltas = rewards[:-1] + self.discount * values[1:] - values[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas,\n",
    "                                                   self.discount * self.lam)\n",
    "        # Rewards-to-go\n",
    "        self.rew_buf[path_slice] = discount_cumsum(rewards,\n",
    "                                                   self.discount)[:-1]\n",
    "        self.start_ptr = self.ptr\n",
    "\n",
    "    def get(self, device):\n",
    "        ''' Empties the buffer into something useable for learning '''\n",
    "        if self.ptr != self.size:\n",
    "            print('ERROR: buffer not full')\n",
    "            return\n",
    "        self.ptr = 0\n",
    "        self.start_ptr = 0\n",
    "        # advantage normalization (LOOK THIS UP!)\n",
    "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / (adv_std + 1e-8)\n",
    "        data = {'obs': self.obs_buf, 'act': self.act_buf, 'ret': self.ret_buf,\n",
    "                'adv': self.adv_buf, 'logp': self.logp_buf}\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32, device=device) for k, v in data.items()}\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nodes_per_layer, activation='relu', batch_norm=True):\n",
    "        '''\n",
    "        A basic multi-layered perceptron using PyTorch.\n",
    "        Parameters\n",
    "        ----------\n",
    "        nodes_per_layer : List[int]\n",
    "            number of nodes per layer in the network\n",
    "        activation : str, optional\n",
    "            description of activation to use in between layers\n",
    "            supports: {'sigmoid', 'relu', 'tanh'}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        activ_func = None\n",
    "        if activation == 'relu':\n",
    "            activ_func = nn.ReLU()\n",
    "        elif activation == 'sigmoid':\n",
    "            activ_func = nn.Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            activ_func = nn.Tanh()\n",
    "        self.mlp = nn.Sequential()\n",
    "        # game state is 2D tensor but we need 1D tensor\n",
    "        nodes_per_layer[0] = nodes_per_layer[0] ** 2\n",
    "        # add layers\n",
    "        for i in range(1, len(nodes_per_layer)-1):\n",
    "            self.mlp.append(nn.Conv2d(nodes_per_layer[i-1],\n",
    "                                      nodes_per_layer[i], 3, padding=1))\n",
    "            if batch_norm:\n",
    "                self.mlp.append(nn.BatchNorm2d(nodes_per_layer[i]))\n",
    "            if activ_func is not None and i != len(nodes_per_layer)-1:\n",
    "                self.mlp.append(activ_func)\n",
    "        self.mlp.append(nn.Flatten())\n",
    "        self.mlp.append(nn.Linear(4 * 4 * nodes_per_layer[-2], nodes_per_layer[-1]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # comes in as (4, 4) tensor for action selection\n",
    "        # comes in as (N, 4, 4) for update\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0).unsqueeze(0)\n",
    "        return self.mlp(x)\n",
    "\n",
    "class ActorCriticMLP:\n",
    "    ''' Actor/Critic that performs actions and makes value estimates '''\n",
    "    def __init__(self, obs_dim, act_dim):\n",
    "        self.actor = MLP([1, 16, 8, act_dim], activation='tanh')\n",
    "        self.critic = MLP([1, 16, 8, 1], activation='tanh')\n",
    "\n",
    "    def distribution(self, obs, mask=None):\n",
    "        ''' Returns the current policy distribution over the observation '''\n",
    "        logits = self.actor(obs)\n",
    "        if mask is not None:\n",
    "            logits = logits.masked_fill(~mask, float('-inf'))\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def policy(self, obs, act=None, mask=None):\n",
    "        ''' Returns an action given the observation '''\n",
    "        pi = self.distribution(obs, mask=mask)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a\n",
    "\n",
    "    def value(self, obs):\n",
    "        ''' Returns the perceived value of the observation '''\n",
    "        return self.critic(obs)\n",
    "\n",
    "    def step(self, obs, mask=None):\n",
    "        ''' Returns the action, value, and logp_a for the observation '''\n",
    "        pi, _ = self.policy(obs, mask=mask)\n",
    "        a = pi.sample()\n",
    "        logp = pi.log_prob(a)\n",
    "        v = self.value(obs)\n",
    "        return a.item(), v.item(), logp.item()\n",
    "\n",
    "    def eval(self):\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "    \n",
    "    def train(self):\n",
    "        self.actor.train()\n",
    "        self.critic.train()\n",
    "    \n",
    "    def to(self, device):\n",
    "        self.actor.to(device)\n",
    "        self.critic.to(device)\n",
    "\n",
    "class VPG:\n",
    "    ''' Vanilla Policy Gradient Algorithm '''\n",
    "    def __init__(self, buffer_size=500, discount=0.99, pi_lr=0.0003, v_lr=0.001, lam=0.97):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.discount = discount\n",
    "        self.pi_lr = pi_lr\n",
    "        self.v_lr = v_lr\n",
    "        self.lam = lam\n",
    "\n",
    "    def compute_loss_pi(self, data):\n",
    "        obs = data['obs']\n",
    "        act = data['act']\n",
    "        adv = data['adv']\n",
    "        logp_old = data['logp']\n",
    "\n",
    "        pi, logp = self.ac.policy(obs, act=act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        return loss_pi\n",
    "\n",
    "    def compute_loss_val(self, data):\n",
    "        obs = data['obs']\n",
    "        ret = data['ret']\n",
    "        return ((self.ac.value(obs) - ret) ** 2).mean()\n",
    "\n",
    "    def update(self):\n",
    "        ''' Updates policy and value parameters via backprop '''\n",
    "        data = self.buffer.get(self.device)\n",
    "\n",
    "        self.pi_optim.zero_grad()\n",
    "        pi_loss = self.compute_loss_pi(data)\n",
    "        pi_loss.backward()\n",
    "        self.pi_optim.step()\n",
    "\n",
    "        for i in range(self.train_v_iters):\n",
    "            self.v_optim.zero_grad()\n",
    "            v_loss = self.compute_loss_val(data)\n",
    "            v_loss.backward()\n",
    "            self.v_optim.step()\n",
    "\n",
    "    def train(self, env_func, epochs=250, train_v_iters=80):\n",
    "        ''' Train an agent on the given environment '''\n",
    "        all_ep_returns = []\n",
    "        all_ep_scores = []\n",
    "        self.device = 'cpu'\n",
    "        print(f'Using device: {self.device}')\n",
    "        env = env_func()\n",
    "        self.buffer = TrajectoryBuffer(env.observation_space.shape,\n",
    "                                       env.action_space.shape,\n",
    "                                       self.buffer_size,\n",
    "                                       discount=self.discount,\n",
    "                                       lam=self.lam)\n",
    "        self.ac = ActorCriticMLP(env.observation_space.shape[0],\n",
    "                                 env.action_space.n)\n",
    "        self.ac.to(self.device)\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.pi_optim = Adam(self.ac.actor.parameters(), lr=self.pi_lr)\n",
    "        self.v_optim = Adam(self.ac.critic.parameters(), lr=self.v_lr)\n",
    "        writer = SummaryWriter()\n",
    "        o = env.reset()\n",
    "        ep_ret = 0\n",
    "        ep_len = 0\n",
    "        action_mask = torch.ones((4,), dtype=torch.bool, device=self.device)\n",
    "        for k in tqdm(range(epochs), total=epochs):\n",
    "            for t in range(self.buffer_size):\n",
    "                with torch.no_grad():\n",
    "                    self.ac.eval()\n",
    "                    a, v, logp = self.ac.step(torch.as_tensor(o, dtype=torch.float32,\n",
    "                                                              device=self.device),\n",
    "                                              mask=action_mask)\n",
    "                    self.ac.train()\n",
    "\n",
    "                next_o, r, done, info = env.step(a)\n",
    "                action_mask = torch.as_tensor(info['action_mask'], dtype=torch.bool,\n",
    "                                              device=self.device)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "                \n",
    "                self.buffer.store(o, a, r, logp, v)\n",
    "                o = next_o\n",
    "\n",
    "                buffer_full = t == self.buffer_size - 1\n",
    "\n",
    "                if done or buffer_full:\n",
    "                    if buffer_full and not done:\n",
    "                        with torch.no_grad():\n",
    "                            self.ac.eval()\n",
    "                            _, v, _ = self.ac.step(torch.as_tensor(o, dtype=torch.float32,\n",
    "                                                                   device=self.device),\n",
    "                                                   mask=action_mask)\n",
    "                            self.ac.train()\n",
    "                    else:\n",
    "                        v = 0.0\n",
    "                    if done:\n",
    "                        ep_score = env.score\n",
    "                        all_ep_returns.append(ep_ret)\n",
    "                        all_ep_scores.append(ep_score)\n",
    "                        writer.add_scalar(\"Return\", ep_ret, k)\n",
    "                        writer.add_scalar(\"Score\", ep_score, k)\n",
    "                        o = env.reset()\n",
    "                        ep_ret = 0\n",
    "                        ep_len = 0\n",
    "                        action_mask = torch.ones((4,), dtype=torch.bool, device=self.device)\n",
    "                    self.buffer.finish_trajectory(last_val=v)\n",
    "            self.update()\n",
    "            writer.flush()\n",
    "        writer.close()\n",
    "        return all_ep_returns, all_ep_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a019b3",
   "metadata": {},
   "source": [
    "### `score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9964f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                             | 27/50000 [00:17<8:51:45,  1.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2884/1096566869.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0menv_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mEnv2048\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mscore_vpg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVPG\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpi_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mscore_ep_returns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_ep_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore_vpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_v_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_v_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2884/2932046921.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, env_func, epochs, train_v_iters)\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mac\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                     a, v, logp = self.ac.step(torch.as_tensor(o, dtype=torch.float32,\n\u001b[0m\u001b[0;32m    263\u001b[0m                                                               device=self.device),\n\u001b[0;32m    264\u001b[0m                                               mask=action_mask)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2884/2932046921.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, obs, mask)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mlogp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\thoma\\anaconda3\\envs\\ailab\\lib\\site-packages\\torch\\distributions\\categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "discount = 1.0\n",
    "lam = 0.97\n",
    "buffer_size = 750\n",
    "pi_lr = 0.03\n",
    "v_lr = 0.01\n",
    "train_v_iters = 20\n",
    "epochs = 50000\n",
    "env_func = lambda : Env2048(size=4, reward_type='score')\n",
    "score_vpg = VPG(buffer_size=buffer_size, discount=discount, pi_lr=pi_lr, v_lr=v_lr, lam=lam)\n",
    "score_ep_returns, score_ep_scores = score_vpg.train(env_func, epochs=epochs, train_v_iters=train_v_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dabad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(score_ep_returns)), score_ep_returns)\n",
    "plt.title('Episode return curve for `score`')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04475c9c",
   "metadata": {},
   "source": [
    "### `survival`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1b5899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [07:41<00:00,  2.17it/s]\n"
     ]
    }
   ],
   "source": [
    "discount = 1.0\n",
    "lam = 0.97\n",
    "buffer_size = 750\n",
    "pi_lr = 0.00003\n",
    "v_lr = 0.0001\n",
    "train_v_iters = 1\n",
    "epochs = 1000\n",
    "env_func = lambda : Env2048(size=4, reward_type='survival')\n",
    "survival_vpg = VPG(buffer_size=buffer_size, discount=discount, pi_lr=pi_lr, v_lr=v_lr, lam=lam)\n",
    "survival_ep_returns, survival_ep_scores = survival_vpg.train(env_func, epochs=epochs, train_v_iters=train_v_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c33d372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(survival_ep_returns)), survival_ep_returns)\n",
    "plt.title('Episode return curve for `survival`')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd96684",
   "metadata": {},
   "source": [
    "### `milestone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ea0dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                             | 36/1000 [00:25<11:12,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "discount = 0.99\n",
    "lam = 0.97\n",
    "buffer_size = 512\n",
    "pi_lr = 0.0003\n",
    "v_lr = 0.001\n",
    "train_v_iters = 80\n",
    "epochs = 1000\n",
    "env_func = lambda : Env2048(size=4, reward_type='milestone')\n",
    "milestone_vpg = VPG(buffer_size=buffer_size, discount=discount, pi_lr=pi_lr, v_lr=v_lr, lam=lam)\n",
    "milestone_ep_returns, milestone_ep_scores = milestone_vpg.train(env_func, epochs=epochs, train_v_iters=train_v_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60261431",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(milestone_ep_returns)), milestone_ep_returns)\n",
    "plt.title('Episode return curve for `milestone`')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb61f7",
   "metadata": {},
   "source": [
    "## **Results**\n",
    "Here is a plot of game scores. This is the best way to compare methods since episode lengths can be arbitrarily long if the same action that doesn't change state is repeated forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f85a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(score_ep_scores)), score_ep_scores, label='score', alpha=0.5)\n",
    "plt.plot(range(len(survival_ep_scores)), survival_ep_scores, label='survival', alpha=0.5)\n",
    "plt.plot(range(len(milestone_ep_scores)), milestone_ep_scores, label='milestone', alpha=0.5)\n",
    "plt.title('Game scores for training on different reward types')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb38c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'actor': score_vpg.ac.actor.state_dict(),\n",
    "            'critic': score_vpg.ac.critic.state_dict()}, 'score_model.pt')\n",
    "torch.save({'actor': survival_vpg.ac.actor.state_dict(),\n",
    "            'critic': survival_vpg.ac.critic.state_dict()}, 'survival_model.pt')\n",
    "torch.save({'actor': milestone_vpg.ac.actor.state_dict(),\n",
    "            'critic': milestone_vpg.ac.critic.state_dict()}, 'milestone_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61281417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
